---
title: "HW1 keys"
author: "Dr. Xiang Ji @ Tulane"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  # ioslides_presentation: default
  html_document:
    toc: true
    toc_depth: 4
---

```{r}
rm(list = ls()) # clean-up workspace
```

## Q1.

Why not test equality with floating point numbers

```{r}
seven <- .7
two <- .2
one <- .1
```

### a

Let's look at the internal representation:
```{r}
sprintf("%a", c(seven, two, one)) # three numbers (Hexadecimal)
```

- Binary format of 0.7 is $1.01100110\dots 0110\times 2^{-1}$

- Binary format of 0.2 is $1.10011001\dots 1010\times 2^{-3}$ (rounding happens here)

- Binary format of 0.1 is $1.01100110\dots 1010\times 2^{-4}$ (rounding happens here)


### b

```{r}
1 == (seven + two) + one
```

### c

```{r}
1 == seven + (two + one)
```

### d

```{r}
1 == (seven + one) + two
```


### e

```{r}
sprintf("%a", c(seven + two, one))
sprintf("%a", (seven + two) + one)
```

For the partial sum $0.7+0.2$, computer

1. aligns the exponent

2. performs addition, and then

3. normalizes the result.

So we get

$$
\begin{aligned}
 &1.0110 \dots 0110 \times 2^{-1}\\
+&0.0110 \dots 0110 \times 2^{-1}\\
=&1.1100 \dots 1100 \times 2^{-1}\\
\end{aligned}
$$

Not in the aligning exponent step, we lost the last two digits of 0.2.

Next we add the partial sum to 0.1

$$
\begin{aligned}
 &1.1100 \dots 1100 \times 2^{-1}\\
+&0.0011 \dots 0011 \times 2^{-1}\\
=&1.1111 \dots 1111 \times 2^{-1}\\
\end{aligned}
$$

Note again, we lost three digits from 0.1 when aligning exponents.  The final result is a bit off from 1.

```{r}
sprintf("%a", 1)
```

You may work on the other two orders: $0.7 + (0.2 + 0.1)$ and $(0.7 + 0.1) + 0.2$, which lead to exact 1.

```{r}
# 0.7 + (0.2 + 0.1)
sprintf("%a", c(seven, two + one)) # partial sum
sprintf("%a", seven + (two + one))

# (0.7 + 0.1) + 0.2
sprintf("%a", c(seven + one, two)) # partial sum
sprintf("%a", (seven + one) + two)
```


## Q2

This is simply the polynomial $y = (x - 1) ^ 7$ expanded.
$y$ values are very close to 0 when $x$ are close to 1.
This calculation is taking differences of very close numbers and losing precision (catastrophic cancellation 2).
The results donâ€™t appear like polynomials due to round-off errors.

```{r}
library(ggplot2)
library("tidyverse")
x <- seq(from = 0.988, to = 1.012, by = 0.001)
y1 <- x^7 - 7 * x^6 + 21 * x^5 - 35 * x^4 + 35 * x^3 - 21 * x^2 + 7 * x - 1 
q3.df <- tibble(x = x, y1 = y1)
ggplot(data = q3.df) +
  geom_point(mapping = aes(x = x, y = y1))
```

This avoids catastrophic cancellation and the result looks nicer.

```{r}
y2 <- (x - 1)^7
add_column(q3.df, y2 = y2)
ggplot(data = q3.df) +
  geom_point(mapping = aes(x = x, y = y2))
```


## Q3

### a

```{r}
u <- c(1, 2, 3, 3, 2, 1)
d <- as.numeric(t(u) %*% u)
d
```

```{r}
# another way
d <- as.numeric(crossprod(u))
d
```

```{r}
# Compute U = I - (2/d) u u' where d = u'u.
U <- diag(6) - (2 / d) * u %*% t(u)
U
```

```{r}
# another way (may be faster) to compute outer product
U <- diag(6) - (2 / d) * tcrossprod(u)
U
```


```{r}
# yet another way to compute outer product
U <- diag(6) - (2 / d) * outer(u, u)
U
```

### b

```{r}
# Let M = U*U
M <- U %*% U
# Find the largest off-diagonal elements of U*U.
max(M[col(M) != row(M)])
```

```{r}
# Find the smallest off-diagonal elements of U*U.
min(M[col(M) != row(M)])
```

### c

```{r}
# two ways to find the largest diagonal elements of U*U
max(diag(M))
max(M[col(M) == row(M)])

# two ways to find the smallest diagonal elements of U*U
min(diag(M))
min(M[col(M) == row(M)])
```

### d

```{r}
# Compute U*u
U %*% u
```

### e

```{r}
# Compute max_j sum_i abs(U(i,j))
max(colSums(abs(U)))
```

### f

```{r}
# Print the third row of U
U[3, ]
```


### g

```{r}
# Print the elements of the second column below the diagonal
U[3:dim(U)[1], 2]
```


### h

```{r}
# Let A be the first three columns of U
A <- U[, 1:3]
# Compute P = AA'
P <- A %*% t(A)
P

# another (preferred) way to compute outer product
P <- tcrossprod(A)
P
```

### i

```{r}
# Compute P*P-P
P%*%P-P
```


### j

```{r}
# Let B be the last three columns of U
B <- U[, 4:6]
# Compute Q = BB'
Q <- B %*% t(B)
Q
# another way to compute outer product
Q <- tcrossprod(B)
Q
```

### k

```{r}
# Compute Q*Q-Q
Q%*%Q-Q
```

### l

```{r}
# Compute P+Q
P+Q
```

## Q4

```{r}
# Read in the matrix in the file 'oringp.dat'.
# The columns are flight number, date, number of O-rings, number failed,
# and temperature at launch.
inputFile <- "http://tulane-math7360.github.io/HW/HW1/oringp.dat"
inputData <- read.table(file = inputFile,
                        col.names = c("flightNumber", "date", "numberOrings", "numberFailed", "tempAtLaunch"))
str(inputData)
inputData <- read_table(file = inputFile, 
                        col_names = c("flightNumber", "date", "numberOrings", "numberFailed", "tempAtLaunch"))
# Compute the correlation between number of failures and temperature at launch, 
# deleting the last, missing observation (the disaster). 
cor(inputData$numberFailed[-24], inputData$tempAtLaunch[-24])
```

## Q5 from [Advanced R](http://adv-r.had.co.nz/Functions.html)

Quiz questions 1, 2, 4, 5

## Q6

### a

```{r}
create.matrix.A <- function(n) {
  # create a matrix with all 0
  B <- matrix(0, n, n)
  # Let the n*n matrix A have elements A(i,j)=1/(|i-j|+1)
  A <- 1 / (abs(row(B) - col(B)) + 1)
  return(A)
}
```

### b

```{r}
A <- create.matrix.A(10)
A
```

### c

```{r}
R <- chol(A)
R
```

### d

```{r}
# two ways to find determinant of A; first way is much more efficient
system.time(prod(diag(R))^2)

system.time(det(A))
